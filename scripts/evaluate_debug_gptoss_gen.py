"""
RAGASãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆgpt-oss:20bç”Ÿæˆ + ELYZAè©•ä¾¡ç‰ˆï¼‰
å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã”ã¨ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚³ã‚¢ã¨è¨ˆç®—éç¨‹ã‚’è¡¨ç¤º
å›ç­”ç”Ÿæˆã«gpt-oss:20bã€RAGASè©•ä¾¡ã«ELYZA-JP-8Bã‚’ä½¿ç”¨
"""
import json
import os
import weaviate
from urllib.parse import urlparse
from typing import List, Dict
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import track, Progress
from config import Config
from openai import AsyncOpenAI, OpenAI
from ragas.llms import llm_factory
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    # answer_relevancy,  # RAGAS 0.4ã¨ã®äº’æ›æ€§å•é¡Œã®ãŸã‚é™¤å¤–
    context_recall,
    context_precision,
)
from datasets import Dataset

console = Console()


class RAGEvaluatorDebug:
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ä»˜ãï¼‰"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        # Weaviateæ¥ç¶š
        parsed_url = urlparse(Config.WEAVIATE_URL)
        host = parsed_url.hostname or "localhost"
        port = parsed_url.port or 8080

        self.weaviate_client = weaviate.connect_to_local(host=host, port=port)
        self.collection_name = Config.WEAVIATE_COLLECTION_NAME

        # Ollamaã‚’OpenAIäº’æ›APIã¨ã—ã¦ä½¿ç”¨ï¼ˆRAGAS 0.4å¯¾å¿œï¼‰
        ollama_base_url = Config.OLLAMA_API_URL.replace("http://ollama:", "http://ollama:")
        if not ollama_base_url.endswith("/v1"):
            ollama_base_url = ollama_base_url.rstrip("/") + "/v1"

        # å›ç­”ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆgpt-oss:20bï¼‰
        self.generation_model = "gpt-oss:20b"
        console.print(f"[green]âœ“ å›ç­”ç”Ÿæˆãƒ¢ãƒ‡ãƒ«: {self.generation_model}[/green]")

        # è©•ä¾¡ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆELYZA-JP-8Bï¼‰
        self.evaluation_model = "elyza-jp-8b"
        console.print(f"[green]âœ“ RAGASè©•ä¾¡ãƒ¢ãƒ‡ãƒ«: {self.evaluation_model}[/green]")

        # AsyncOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆï¼ˆRAGASè©•ä¾¡ç”¨ - ELYZA-JP-8Bï¼‰
        openai_client_eval = AsyncOpenAI(
            api_key="ollama",
            base_url=ollama_base_url,
            timeout=600.0,  # 10åˆ†
            max_retries=3
        )

        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆåŒæœŸç‰ˆ - gpt-oss:20bï¼‰
        self.openai_sync_client = OpenAI(
            api_key="ollama",
            base_url=ollama_base_url,
            timeout=600.0,  # 10åˆ†
            max_retries=3
        )

        # RAGAS 0.4ã®llm_factoryã§LLMä½œæˆï¼ˆè©•ä¾¡ç”¨ã¯ELYZA-JP-8Bï¼‰
        self.llm = llm_factory(
            model=self.evaluation_model,
            provider="openai",
            client=openai_client_eval
        )

    def retrieve_contexts(self, question: str, top_k: int = 3) -> List[str]:
        """
        è³ªå•ã«å¯¾ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢

        Args:
            question: è³ªå•
            top_k: å–å¾—ã™ã‚‹çµæœæ•°

        Returns:
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        collection = self.weaviate_client.collections.get(self.collection_name)

        response = collection.query.near_text(
            query=question,
            limit=top_k
        )

        contexts = []
        for obj in response.objects:
            content = obj.properties.get("content", "")
            contexts.append(content)

        return contexts

    def generate_answer(self, question: str, contexts: List[str]) -> str:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è³ªå•ã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆï¼ˆgpt-oss:20bä½¿ç”¨ï¼‰

        Args:
            question: è³ªå•
            contexts: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        """
        context_text = "\n\n".join(contexts)

        prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context_text}

è³ªå•: {question}

å›ç­”:"""

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç›´æ¥ä½¿ç”¨ã—ã¦å›ç­”ç”Ÿæˆï¼ˆgpt-oss:20bï¼‰
        response = self.openai_sync_client.chat.completions.create(
            model=self.generation_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content

    def run_evaluation_with_debug(self, testset: List[Dict]) -> tuple:
        """
        ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãï¼‰

        Args:
            testset: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ

        Returns:
            (è©•ä¾¡çµæœ, ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ãƒªã‚¹ãƒˆ)
        """
        console.print("[blue]RAGè©•ä¾¡ã‚’å®Ÿè¡Œä¸­ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰...[/blue]\n")

        # å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã«å¯¾ã—ã¦RAGã‚’å®Ÿè¡Œ
        questions = []
        answers = []
        contexts = []
        ground_truths = []
        debug_info = []

        for idx, test_case in enumerate(track(testset, description="Processing"), 1):
            question = test_case["question"]
            ground_truth = test_case["ground_truth"]

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
            retrieved_contexts = self.retrieve_contexts(question, top_k=Config.TOP_K_RESULTS)

            # å›ç­”ç”Ÿæˆï¼ˆgpt-oss:20bï¼‰
            answer = self.generate_answer(question, retrieved_contexts)

            questions.append(question)
            answers.append(answer)
            contexts.append(retrieved_contexts)
            ground_truths.append(ground_truth)

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ä¿å­˜
            debug_info.append({
                "index": idx,
                "question": question,
                "retrieved_contexts": retrieved_contexts,
                "answer": answer,
                "ground_truth": ground_truth,
                "num_contexts": len(retrieved_contexts)
            })

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆRAGAS 0.4ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å½¢å¼ï¼‰
        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truths": ground_truths,
            "reference": ground_truths
        }

        dataset = Dataset.from_dict(data)

        console.print("\n[blue]RAGASè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...[/blue]")

        # RAGASè©•ä¾¡å®Ÿè¡Œï¼ˆanswer_relevancyã‚’é™¤å¤–ï¼‰
        result = evaluate(
            dataset,
            metrics=[
                faithfulness,
                # answer_relevancy,  # äº’æ›æ€§å•é¡Œã®ãŸã‚é™¤å¤–
                context_recall,
                context_precision,
            ],
            llm=self.llm,
        )

        return result, debug_info

    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒ­ãƒ¼ã‚º"""
        self.weaviate_client.close()


def load_testset(testset_path: str = "evaluation/testset.json") -> List[Dict]:
    """
    ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰

    Args:
        testset_path: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹

    Returns:
        ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    with open(testset_path, 'r', encoding='utf-8') as f:
        testset = json.load(f)

    console.print(f"[green]âœ“ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰: {len(testset)}ä»¶[/green]\n")
    return testset


def display_debug_info(debug_info: List[Dict], result):
    """
    ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º

    Args:
        debug_info: ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        result: RAGASè©•ä¾¡çµæœ
    """
    console.print("\n[bold cyan]" + "="*80 + "[/bold cyan]")
    console.print("[bold cyan]è©³ç´°ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°[/bold cyan]")
    console.print("[bold cyan]" + "="*80 + "[/bold cyan]\n")

    # è©•ä¾¡çµæœã‚’DataFrameã«å¤‰æ›
    df = result.to_pandas()

    for i, info in enumerate(debug_info):
        console.print(f"\n[bold yellow]{'='*80}[/bold yellow]")
        console.print(f"[bold yellow]ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ #{info['index']}: {info['question'][:60]}...[/bold yellow]")
        console.print(f"[bold yellow]{'='*80}[/bold yellow]\n")

        # è³ªå•
        console.print(Panel(
            info['question'],
            title="[bold cyan]è³ªå•[/bold cyan]",
            border_style="cyan"
        ))

        # æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        console.print(f"\n[bold green]æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ{info['num_contexts']}ä»¶ï¼‰:[/bold green]")
        for ctx_idx, context in enumerate(info['retrieved_contexts'], 1):
            console.print(f"\n[dim]--- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ {ctx_idx} ---[/dim]")
            console.print(f"[dim]{context[:300]}...[/dim]")

        # LLMã®å›ç­”
        console.print(f"\n[bold blue]LLMã®å›ç­”:[/bold blue]")
        console.print(Panel(
            info['answer'],
            border_style="blue"
        ))

        # ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹
        console.print(f"\n[bold magenta]ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ï¼ˆæœŸå¾…ã•ã‚Œã‚‹æ­£è§£ï¼‰:[/bold magenta]")
        console.print(Panel(
            info['ground_truth'],
            border_style="magenta"
        ))

        # è©•ä¾¡ã‚¹ã‚³ã‚¢
        if i < len(df):
            row = df.iloc[i]
            console.print(f"\n[bold white]è©•ä¾¡ã‚¹ã‚³ã‚¢:[/bold white]")

            table = Table(show_header=True, header_style="bold")
            table.add_column("æŒ‡æ¨™", style="cyan")
            table.add_column("ã‚¹ã‚³ã‚¢", style="green", justify="right")
            table.add_column("èª¬æ˜", style="dim")

            metrics_info = {
                "faithfulness": "å¿ å®Ÿæ€§ï¼ˆå›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ï¼‰",
                # "answer_relevancy": "å›ç­”é–¢é€£æ€§ï¼ˆè³ªå•ã«é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹ï¼‰",
                "context_recall": "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†ç¾ç‡ï¼ˆå¿…è¦ãªæƒ…å ±ã‚’æ¤œç´¢ã§ãã¦ã„ã‚‹ã‹ï¼‰",
                "context_precision": "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç²¾åº¦ï¼ˆæ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªï¼‰",
            }

            for metric, description in metrics_info.items():
                if metric in row:
                    score = row[metric]
                    if isinstance(score, (int, float)) and not (score != score):  # NaNãƒã‚§ãƒƒã‚¯
                        table.add_row(metric, f"{score:.4f}", description)
                    else:
                        table.add_row(metric, "N/A", description)

            console.print(table)

        console.print("\n")


def display_summary(result):
    """
    è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º

    Args:
        result: RAGASè©•ä¾¡çµæœ
    """
    console.print("\n[bold cyan]" + "="*80 + "[/bold cyan]")
    console.print("[bold cyan]è©•ä¾¡ã‚µãƒãƒªãƒ¼[/bold cyan]")
    console.print("[bold cyan]" + "="*80 + "[/bold cyan]\n")

    # DataFrameã«å¤‰æ›
    df = result.to_pandas()

    # å…¨ä½“ã®å¹³å‡ã‚¹ã‚³ã‚¢
    table = Table(title="å…¨ä½“å¹³å‡ã‚¹ã‚³ã‚¢", show_header=True, header_style="bold cyan")
    table.add_column("ãƒ¡ãƒˆãƒªã‚¯ã‚¹", style="cyan")
    table.add_column("å¹³å‡ã‚¹ã‚³ã‚¢", style="green", justify="right")
    table.add_column("èª¬æ˜", style="dim")

    metrics_info = {
        "faithfulness": ("å¿ å®Ÿæ€§", "å›ç­”ãŒæ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹"),
        # "answer_relevancy": ("å›ç­”é–¢é€£æ€§", "å›ç­”ãŒè³ªå•ã«é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹"),
        "context_recall": ("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†ç¾ç‡", "å¿…è¦ãªæƒ…å ±ã‚’æ¤œç´¢ã§ãã¦ã„ã‚‹ã‹"),
        "context_precision": ("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç²¾åº¦", "æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ª"),
    }

    avg_scores = {}
    for metric in metrics_info.keys():
        if metric in df.columns:
            valid_scores = df[metric].dropna()
            if len(valid_scores) > 0:
                avg_score = valid_scores.mean()
                avg_scores[metric] = avg_score
                label, description = metrics_info[metric]
                table.add_row(label, f"{avg_score:.4f}", description)

    console.print(table)

    # ç·åˆè©•ä¾¡
    if avg_scores:
        overall_avg = sum(avg_scores.values()) / len(avg_scores)
        console.print(f"\n[bold]ç·åˆã‚¹ã‚³ã‚¢: {overall_avg:.4f}[/bold]")

        if overall_avg >= 0.8:
            console.print("[green]âœ“ å„ªç§€ãªRAGã‚·ã‚¹ãƒ†ãƒ ã§ã™ï¼[/green]")
        elif overall_avg >= 0.6:
            console.print("[yellow]âš  æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™[/yellow]")
        else:
            console.print("[red]âœ— å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™[/red]")


def save_debug_log(debug_info: List[Dict], result, output_path: str = "evaluation/debug_log.json"):
    """
    ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜

    Args:
        debug_info: ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        result: RAGASè©•ä¾¡çµæœ
        output_path: ä¿å­˜å…ˆãƒ‘ã‚¹
    """
    # è©•ä¾¡çµæœã‚’DataFrameã«å¤‰æ›
    df = result.to_pandas()

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã¨è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’çµåˆ
    full_log = []
    for i, info in enumerate(debug_info):
        log_entry = info.copy()

        if i < len(df):
            row = df.iloc[i]
            log_entry["scores"] = {
                "faithfulness": float(row["faithfulness"]) if "faithfulness" in row and not (row["faithfulness"] != row["faithfulness"]) else None,
                # "answer_relevancy": float(row["answer_relevancy"]) if "answer_relevancy" in row and not (row["answer_relevancy"] != row["answer_relevancy"]) else None,
                "context_recall": float(row["context_recall"]) if "context_recall" in row and not (row["context_recall"] != row["context_recall"]) else None,
                "context_precision": float(row["context_precision"]) if "context_precision" in row and not (row["context_precision"] != row["context_precision"]) else None,
            }

        full_log.append(log_entry)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(full_log, f, ensure_ascii=False, indent=2)

    console.print(f"\n[green]âœ“ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜: {output_path}[/green]")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    console.print("[bold cyan]RAGASã«ã‚ˆã‚‹RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ï¼ˆgpt-oss:20bç”Ÿæˆ + ELYZAè©•ä¾¡ç‰ˆãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰[/bold cyan]\n")
    console.print("[yellow]ğŸ“Œ å›ç­”ç”Ÿæˆ: gpt-oss:20b (å¤§å‹æ±ç”¨ãƒ¢ãƒ‡ãƒ«)[/yellow]")
    console.print("[yellow]ğŸ“Œ RAGASè©•ä¾¡: ELYZA-JP-8B (æ—¥æœ¬èªç‰¹åŒ–ã€é«˜é€Ÿ)[/yellow]\n")

    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
    testset = load_testset()

    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = RAGEvaluatorDebug()
    result, debug_info = evaluator.run_evaluation_with_debug(testset)
    evaluator.close()

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    display_debug_info(debug_info, result)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    display_summary(result)

    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜ï¼ˆgpt-ossç”Ÿæˆç‰ˆï¼‰
    save_debug_log(debug_info, result, output_path="evaluation/debug_log_gptoss_gen.json")

    console.print("\n[bold green]è©•ä¾¡å®Œäº†ï¼[/bold green]")
    console.print("[dim]çµæœã¯evaluation/debug_log_gptoss_gen.jsonã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ[/dim]")


if __name__ == "__main__":
    main()
